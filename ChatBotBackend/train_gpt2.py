#!/usr/bin/env python3
# train_gpt2.py  â€“Â with DataCollatorForLanguageModeling

from datasets import load_from_disk
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    TrainerCallback,
    DataCollatorForLanguageModeling,
)

# â”€â”€â”€â”€â”€ Config â”€â”€â”€â”€â”€
MODEL_NAME  = "gpt2"
DATA_PATH   = "tokenized_data"   # Directory from prepare_dataset.py
OUTPUT_DIR  = "gpt2-chatbot"

# â”€â”€ Callback to print training loss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class LossPrinter(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs and "loss" in logs:
            print(f"step {state.global_step:>4} | loss {logs['loss']:.4f}")

# â”€â”€ 1. Load & extend tokenizer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("âš™ï¸  Loading tokenizer and adding special tokensâ€¦")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

specials = {}
if "<|startoftext|>" not in tokenizer.get_vocab():
    specials["bos_token"] = "<|startoftext|>"
if "<|endoftext|>" not in tokenizer.get_vocab():
    specials["eos_token"] = "<|endoftext|>"
if tokenizer.pad_token is None:
    specials["pad_token"] = tokenizer.eos_token

if specials:
    tokenizer.add_special_tokens(specials)
    print(f"Added specials: {specials}")
else:
    print("No new special tokens added.")

# â”€â”€ 2. Load model and resize token embeddings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("âš™ï¸  Loading model and resizing token embeddingsâ€¦")
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
model.resize_token_embeddings(len(tokenizer))

# â”€â”€ 3. Load tokenized dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"âš™ï¸  Loading tokenized dataset from '{DATA_PATH}'â€¦")
dataset = load_from_disk(DATA_PATH)

# Debug: check for outâ€‘ofâ€‘range token IDs
max_id = max(sum(dataset["input_ids"], []))
print(f"ğŸ” Max token id: {max_id}, Vocab size: {len(tokenizer)}")
assert max_id < len(tokenizer), "Token id out of range â€“Â check tokenizer & dataset!"

# â”€â”€ 4. Data collator (creates labels on the fly) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("âš™ï¸  Setting up DataCollatorForLanguageModelingâ€¦")
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,            # causal LM => no masking
)

# â”€â”€ 5. Training arguments â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
training_args = TrainingArguments(
    output_dir             = OUTPUT_DIR,
    overwrite_output_dir   = True,
    per_device_train_batch_size = 1,
    num_train_epochs       = 5,      # more epochs for small data
    learning_rate          = 5e-5,
    logging_steps          = 10,
    save_steps             = 500,
    save_total_limit       = 2,
    dataloader_pin_memory  = False,
    remove_unused_columns  = False,  # keep input_ids & attention_mask for collator
)

# â”€â”€ 6. Initialize Trainer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
trainer = Trainer(
    model           = model,
    args            = training_args,
    train_dataset   = dataset,
    data_collator   = data_collator,
    callbacks       = [LossPrinter()],
)

# â”€â”€ 7. Start training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("âš™ï¸  Starting trainingâ€¦")
trainer.train()

# â”€â”€ 8. Save the fineâ€‘tuned model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print(f"âš™ï¸  Saving fineâ€‘tuned model to '{OUTPUT_DIR}'â€¦")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print("âœ… Training complete, model saved.")
